[run]
name = lr-annealing-target-network
seed = 42
gpu_id = 0

[env]
num_problems_per_module : 10 ** 6
validation_percentage : 0.2
max_sequence_length : 375
vocab_size : 250
max_difficulty : 0
univariate_differentiation : True
num_environments = 50
tokenizer_filepath = "environment/tokenization/tokenizer.model"
max_formal_elements = 13
max_num_nodes = 10

data_dirpath = "mathematics_dataset-v1.0/train-easy"
selected_filenames = [
                     ;     'numbers__is_factor.txt',
                     ;     'numbers__is_prime.txt',
                     ;     'numbers__list_prime_factors.txt',
                     'calculus__differentiate.txt',
                     ;     'polynomials__evaluate.txt',
                     ;     'numbers__div_remainder.txt',
                     ;     'numbers__gcd.txt',
                     ;     'numbers__lcm.txt',
                     ;     'algebra__linear_1d.txt',
                     ;     'algebra__polynomial_roots.txt',
                     ;     'algebra__linear_2d.txt',
                     ;     'algebra__linear_1d_composed.txt',
                     ;     'algebra__linear_2d_composed.txt',
                     ;     'algebra__polynomial_roots_composed.txt',
                     ;     'calculus__differentiate_composed.txt',
                     ;     'numbers__div_remainder_composed.txt',
                     ;     'numbers__gcd_composed.txt',
                     ;     'numbers__is_factor_composed.txt',
                     ;     'numbers__is_prime_composed.txt',
                     ;     'numbers__lcm_composed.txt',
                     ;     'numbers__list_prime_factors_composed.txt',
                     ;     'polynomials__evaluate_composed.txt',
                     ;     'polynomials__compose.txt'
                     ]

# used only to initialize the tokenizer
types = [
        "Arbitrary",
        "Equation",
        "Function",
        "Expression",
        "Variable",
        "Value",
        "Rational"
        ]

operators = [
            "lookup_value",
            "solve_system",
            "append",
            "append_to_empty_list",
            "make_equation",
            "lookup_value_equation",
            "extract_isolated_variable",
            "substitution_left_to_right",
            "factor",
            "differentiate",
            "differentiate_wrt",
            "simplify",
            "make_function",
            "replace_arg",
            "mod",
            "gcd",
            "divides",
            "is_prime",
            "lcm",
            "lcd",
            "prime_factors",
            "evaluate_function",
            "not_op"
            ]

[model]
nhead = 8
nhid = 512
nlayers = 2
model_filepath = None
# model_type can be 'value' or 'policy'
model_type = 'value'
action_embedding_size = 32
# Action Encoder
lstm_hidden_size = 128
lstm_nlayers = 1

[train]
num_epochs = 20000
replay_buffer_size = 100000
random_exploration_trajectory_cache_filepath = "mathematics_dataset-v1.0/random_exploration_trajectory_cache.sqlite"
model_exploration_trajectory_cache_filepath = "mathematics_dataset-v1.0/model_exploration_trajectory_cache.sqlite"

# fill buffer
buffer_threshold = 50
positive_to_negative_ratio = 1
fill_buffer_mode = 'anything'
num_batches_until_fill_buffer = 10000

# training
batch_size = 128
lr = 0.001
max_grad_norm = 0.5
dropout = 0.2
epsilon = 0.2
gamma = 0.9
min_saved_steps_until_training = 10000
batches_per_epoch = 5

# LR scheduler
factor = 0.5
patience = 50
min_lr = 0.00001
#max_lr = 0.01
#total_steps = 1000
#final_div_factor = 0.01

# replay priority
prioritization_exponent = 2
default_replay_buffer_priority = 1

# target network
use_target_network = True
batches_per_target_network_update = 500
batches_until_target_network = 20000

# eval
n_required_validation_episodes = 100
batches_per_eval = 100