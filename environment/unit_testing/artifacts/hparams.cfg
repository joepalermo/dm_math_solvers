[run]
name = unit-testing
seed = 0
gpu_id = 0

[env]
num_problems_per_module: 10 ** 6
validation_percentage: 0
max_sequence_length: 200
vocab_size: 200
max_difficulty: 0
univariate_differentiation: False
num_environments = 32
tokenizer_filepath = "artifacts/tokenizer.model"
max_formal_elements = 13
max_num_nodes = 10
data_dirpath = "artifacts"
selected_filenames = ["short_problems.txt"]
operators = [
            "lv",
            "ss",
            "ap",
            "ape",
            "meq",
            "lve",
            "eiv",
            "slr",
            "fac",
            "df",
            "dfw",
            "sy",
            "mfn",
            "ra",
            "mod",
            "gcd",
            "md0",
            "ip",
            "lcm",
            "lcd",
            "pf",
            "fa",
            "nt"
            ]

[model]
nhead = 8
nhid = 512
nlayers = 1
dropout = 0.1
model_filepath = None

[train]
batch_size = 32
buffer_threshold = 32
positive_to_negative_ratio = 1
lr = 0.01
n_required_validation_episodes = 2000
max_grad_norm = 0.5
#'positive_only' or 'balanced'
mode = 'positive_only'
use_replay_buffer = False

num_buffers = 1000000000
fill_buffer_max_steps = 1000000000
batches_per_eval = 10
batches_per_train = 10

#scheduler:
factor = 0.2
patience = 15
min_lr = 0.01